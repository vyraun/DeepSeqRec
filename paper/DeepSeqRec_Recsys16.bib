@inproceedings{Cho2014,
abstract = {In this paper, we propose a novel neural network model called RNN Encoder--Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder--Decoder as an additional feature in the existing linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
archivePrefix = {arXiv},
arxivId = {1406.1078},
author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
doi = {10.3115/v1/D14-1179},
eprint = {1406.1078},
file = {:Users/haroldsoh/Documents/Mendeley Desktop/Merri/Unknown/1406.1078.pdf:pdf},
booktitle = {EMNLP},
keywords = {decoder,for statistical machine translation,rning phrase representations using,rnn encoder},
pages = {1724--1734},
title = {{Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation}},
url = {http://arxiv.org/abs/1406.1078},
year = {2014}
}

@article{Shani2012,
abstract = {Typical Recommender systems adopt a static view of the recommendation process and treat it as a prediction problem. We argue that it is more appropriate to view the problem of generating recommendations as a sequential decision problem and, consequently, that Markov decision processes (MDP) provide a more appropriate model for Recommender systems. MDPs introduce two benefits: they take into account the long-term effects of each recommendation, and they take into account the expected value of each recommendation. To succeed in practice, an MDP-based Recommender system must employ a strong initial model; and the bulk of this paper is concerned with the generation of such a model. In particular, we suggest the use of an n-gram predictive model for generating the initial MDP. Our n-gram model induces a Markov-chain model of user behavior whose predictive accuracy is greater than that of existing predictive models. We describe our predictive model in detail and evaluate its performance on real data. In addition, we show how the model can be used in an MDP-based Recommender system.},
archivePrefix = {arXiv},
arxivId = {1301.0600},
author = {Shani, Guy and Brafman, Ronen I. and Heckerman, David},
eprint = {1301.0600},
file = {:Users/haroldsoh/Documents/Mendeley Desktop/Shani, Heckerman, Brafman/Unknown/shani05a.pdf:pdf},
isbn = {1533-7928},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
keywords = {commercial applications,learning,markov decision processes,recommender systems},
pages = {1265--1295},
title = {{An MDP-based Recommender System}},
url = {http://arxiv.org/abs/1301.0600},
volume = {6},
year = {2012}
}


@article{Yap2012,
author = {Yap, Ghim Eng and Li, Xiao Li and Yu, Philip S.},
doi = {10.1007/978-3-642-29035-0_4},
file = {:Users/haroldsoh/Documents/Mendeley Desktop/Yap, Li, Yu/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/DASFAA11{\_}Recommendation.pdf:pdf},
isbn = {9783642290343},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 2},
pages = {48--64},
title = {{Effective next-items recommendation via personalized sequential pattern mining}},
volume = {7239 LNCS},
year = {2012}
}


@inproceedings{Sedhain2015,
 author = {Sedhain, Suvash and Menon, Aditya Krishna and Sanner, Scott and Xie, Lexing},
 title = {AutoRec: Autoencoders Meet Collaborative Filtering},
 booktitle = {Proceedings of the 24th International Conference on World Wide Web},
 series = {WWW '15 Companion},
 year = {2015},
 isbn = {978-1-4503-3473-0},
 location = {Florence, Italy},
 pages = {111--112},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/2740908.2742726},
 doi = {10.1145/2740908.2742726},
 acmid = {2742726},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {autoencoders, collaborative filtering, recommender systems},
} 


@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik and Ba, Jimmy},
booktitle = {{International Conference on Learning Representations}},
eprint = {1412.6980},
file = {:Users/haroldsoh/Documents/Mendeley Desktop/Kingma, Ba/International conference on Learning Representations/1412.6980v8.pdf:pdf},
pages = {1--15},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980$\backslash$nhttp://www.arxiv.org/pdf/1412.6980.pdf},
year = {2015}
}


@inproceedings{Rendle2010,
abstract = {Recommender systems are an important component of many websites. Two of the most popular approaches are based on matrix factorization (MF) and Markov chains (MC). MF methods learn the general taste of a user by factorizing the matrix over observed user-item preferences. On the other hand, MC methods model sequential behavior by learning a transition graph over items that is used to predict the next action based on the recent actions of a user. In this paper, we present a method bringing both approaches together. Our method is based on personalized transition graphs over un- derlying Markov chains. That means for each user an own transition matrix is learned â€“ thus in total the method uses a transition cube. As the observations for estimating the transitions are usually very limited, our method factorizes the transition cube with a pairwise interaction model which is a special case of the Tucker Decomposition. We show that our factorized personalized MC (FPMC) model sub- sumes both a common Markov chain and the normal matrix factorization model. For learning the model parameters, we introduce an adaption of the Bayesian Personalized Ranking (BPR) framework for sequential basket data. Empirically, we show that our FPMC model outperforms both the com- mon matrix factorization and the unpersonalized MC model both learned with and without factorization. Categories},
author = {Rendle, Steffen and Freudenthaler, Christoph and Schmidt-Thieme, Lars},
doi = {10.1145/1772690.1772773},
file = {:Users/haroldsoh/Documents/Mendeley Desktop/Rendle, Freudenthaler, Schmidt-Thieme/Proceedings of the 19th international conference on World wide web - WWW '10/RendleFreudenthaler2010-FPMC.pdf:pdf},
isbn = {9781605587998},
issn = {1469493X},
booktitle = {WWW '10},
pages = {811},
pmid = {21975771},
title = {{Factorizing personalized Markov chains for next-basket recommendation}},
url = {http://portal.acm.org/citation.cfm?doid=1772690.1772773},
year = {2010}
}


@article{Hochreiter1997,
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
archivePrefix = {arXiv},
arxivId = {1206.2944},
author = {Hochreiter, Sepp and Hochreiter, S and Schmidhuber, J{\"{u}}rgen and Schmidhuber, J},
doi = {10.1162/neco.1997.9.8.1735},
eprint = {1206.2944},
file = {:Users/haroldsoh/Documents/Mendeley Desktop/Hochreiter et al/Neural computation/Hochreiter97{\_}lstm.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Learning,Memory,Memory, Short-Term,Models,Models, Neurological,Models, Psychological,Nerve Net,Nerve Net: physiology,Neural Networks (Computer),Neurological,Psychological,Short-Term,Time Factors},
number = {8},
pages = {1735--80},
pmid = {9377276},
title = {{Long short-term memory.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9377276},
volume = {9},
year = {1997}
}

@inproceedings{jozefowicz2015empirical,
  title={An empirical exploration of recurrent network architectures},
  author={Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  booktitle={ICML},
  pages={2342--2350},
  year={2015}
}

@article{Bengio1994,
abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:Users/haroldsoh/Documents/Mendeley Desktop/Unknown/Unknown/tnn-94-gradient.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {19410093},
journal = {IEEE Transactions on Neural Networks},
number = {2},
pages = {157--166},
pmid = {18267787},
title = {{Learning Long-Term Dependencies with Gradient Descent is Difficult}},
volume = {5},
year = {1994}
}

@online{TFCandidateSampling,
title = {{Candidate Sampling Algorithms Reference}},
howpublished = {\url{https://www.tensorflow.org/extras/candidate_sampling.pdf}},
date = {2015},
urldate = {2016-04-18}
}

@online{Assistmentsdata,
title = {{2015 ASSISTments Skill Builder Data}},
howpublished = {\url{https://sites.google.com/site/assistmentsdata/home/2015-assistments-skill-builder-data}}
date = {2015},
urldate = {2016-04-18}
}

@article{Ye2011,
abstract = {In this paper, we aim to provide a point-of-interests (POI) recommendation service for the rapid growing location-based social networks (LBSNs), e.g., Foursquare, Whrrl, etc. Our idea is to explore user preference, social influence and geographical influence for POI recommendations. In addition to deriving user preference based on user-based collaborative filtering and exploring social influence from friends, we put a special emphasis on geographical influence due to the spatial clustering phenomenon exhibited in user check-in activities of LBSNs. We argue that the geographical influence among POIs plays an important role in user check-in behaviors and model it by power law distribution. Accordingly, we develop a collaborative recommendation algorithm based on geographical influence based on naive Bayesian. Furthermore, we propose a unified POI recommendation framework, which fuses user preference to a POI with social influence and geographical influence. Finally, we conduct a comprehensive performance evaluation over two large-scale datasets collected from Foursquare and Whrrl. Experimental results with these real datasets show that the unified collaborative recommendation approach significantly outperforms a wide spectrum of alternative recommendation approaches.},
author = {Ye, Mao and Yin, Peifeng and Lee, Wang-Chien and Lee, Dik-Lun},
doi = {10.1145/2009916.2009962},
file = {:Users/haroldsoh/Documents/Mendeley Desktop/Ye et al/Unknown/c0b64137a555ed68f40071477e75a87b0e12.pdf:pdf},
isbn = {978-1-4503-0757-4},
journal = {Proceedings of the 34th international ACM SIGIR conference on Research and development in Information},
keywords = {collaborative recommendation,geographical influence,location-based social networks},
pages = {325--334},
title = {{Exploiting geographical influence for collaborative point-of-interest recommendation}},
url = {http://doi.acm.org/10.1145/2009916.2009962},
year = {2011}
}



@inproceedings{Jean2015,
abstract = {Neural machine translation, a recently proposed approach to machine transla-tion based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. De-spite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complex-ity as well as decoding complexity in-crease proportionally to the number of tar-get words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vo-cabulary without increasing training com-plexity. We show that decoding can be efficiently done even with the model hav-ing a very large target vocabulary by se-lecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases out-perform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Fur-thermore, when we use an ensemble of a few models with very large target vo-cabularies, we achieve performance com-parable to the state of the art (measured by BLEU) on both the Englishâ†’German and Englishâ†’French translation tasks of WMT'14.},
archivePrefix = {arXiv},
arxivId = {1412.2007v2},
author = {Jean, S{\'{e}}bastien and Cho, Kyunghyun and Memisevic, Roland and Bengio, Yoshua},
eprint = {1412.2007v2},
file = {:Users/haroldsoh/Documents/Mendeley Desktop/Cho/Unknown/P15-1001.pdf:pdf},
isbn = {9781941643723},
booktitle = {Proc. of the 53rd Annual Meeting of the Assoc. for Comp. Linguistics and the 7th Intl. Joint Conf. on Nat. Language Proc.},
pages = {1--10},
title = {{On Using Very Large Target Vocabulary for Neural Machine Translation}},
url = {http://www.aclweb.org/anthology/P15-1001},
year = {2015}
}

@inproceedings{Breese1998,
abstract = {Correlation, vector similarity, default voting, Inverse user frequency, case amplification Cluster model (native bayes estimated with EM), Bayesian Network Model (decision tree for each item) To evaluate top-N lists they use an exponential decay function to reflecht that a user is more likely to use the items on top of the list (they use a halflife of 5 items) Bayesian networks with decision trees at each node and correlation meth- ods are the best performing algorithms protocols: all but 1, given 2, given 5, given 10 shows learning effect by sampling form training data.},
author = {Breese, John S and Heckerman, David and Kadie, Carl},
doi = {10.1111/j.1553-2712.2011.01172.x},
file = {:Users/haroldsoh/Documents/Mendeley Desktop/Breese, Heckerman, Kadie/Proceedings of the 14th conference on Uncertainty in Artificial Intelligence/tr-98-12.pdf:pdf},
isbn = {155860555X},
issn = {15532712},
booktitle = {UAI'14},
number = {8},
pages = {43--52},
pmid = {21843212},
title = {{Empirical analysis of predictive algorithms for collaborative filtering}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Empirical+Analysis+of+Predictive+Algorithms+for+Collaborative+Filtering{\#}0},
volume = {461},
year = {1998}
}

@inproceedings{Feng2015,
author = {Feng, Shanshan and Li, Xutao and Zeng, Yifeng and Cong, Gao and Chee, Yeow Meng and Yuan, Quan},
file = {:Users/haroldsoh/Documents/Mendeley Desktop/Feng et al/IJCAI International Joint Conference on Artificial Intelligence/IJCAI15-293.pdf:pdf},
booktitle = {IJCAI},
keywords = {Technical Papers â€” Social Networks},
pages = {2069--2075},
title = {{Personalized Ranking Metric Embedding for Next New POI Recommendation}},
year = {2015}
}

@article{Pera2013,
abstract = {Finding books that children/teenagers are interested in these days is a non-trivial task due to the diversity of topics cov- ered in huge volumes of books with varied readability levels. Even though K-12 readers can turn to book recommenders to look for books, the recommended books may not satisfy their personal needs, since they could be beyond/below their readability levels or fail to match their topics of interest. To address these problems, we introduce BReK12, a book rec- ommender that makes personalized suggestions tailored to each K-12 user U based on books available on a social book- marking site that (i) are similar in content to the ones that are known to be of interest to U, (ii) have been bookmarked by users with reading patterns similar to U's, and (iii) can be comprehended by U. BReK12 is an asset to its users, since it suggests books that are appealing to its users and at grade levels that they can cope with, which can increase their reading selection choices and motivate them to read. We have also developed ReLAT, the readability analysis tool employed by BReK12 to determine the grade level of books. ReLAT is novel, compared with existing readability formu- las, since it can predict the grade level of a book even if an excerpt of the book is not available. We have conducted empirical studies which have verified the accuracy of ReLAT in predicting the grade level of a book and the effectiveness of BReK12 over existing baseline recommendation systems. Categories},
author = {Pera, Ms and Ng, Yk},
doi = {10.1145/2507157.2507181},
file = {:Users/haroldsoh/Documents/Mendeley Desktop/Pera/Unknown/p113-pera.pdf:pdf},
isbn = {9781450324090},
journal = {Proceedings of the 7th ACM conference on Recommender systems - RecSys '13},
keywords = {book recommendation system,k-12,readability},
pages = {113--120},
title = {{What to read next?: making personalized book recommendations for K-12 users}},
url = {http://dl.acm.org/citation.cfm?id=2507181},
year = {2013}
}
@article{Wu2013,
abstract = {In this paper, we propose Personalized Markov Embedding (PME), a next-song recommendation strategy for online karaoke users. By modeling the sequential singing behavior, we first embed songs and users into a Euclidean space in which distances between songs and users reflect the strength of their relationships. Then, given each user's last song, we can generate personalized recommendations by ranking the candidate songs according to the embedding. More- over, PME can be trained without any requirement of content infor- mation. Finally, we perform an experimental evaluation on a real world data set provided by ihou.com which is an online karaoke website launched by iFLYTEK, and the results clearly demonstrate the effectiveness of PME.},
author = {Wu, Xiang and Liu, Qi and Chen, Enhong and He, Liang and Lv, Jingsong and Cao, Can and Hu, Guoping},
doi = {10.1145/2507157.2507215},
file = {:Users/haroldsoh/Documents/Mendeley Desktop/Wu et al/Unknown/sys145-wu.pdf:pdf},
isbn = {9781450324090},
journal = {Proceedings of the 7th ACM conference on Recommender systems - RecSys '13},
keywords = {markov embedding,music recommendation,personalization},
pages = {137--140},
title = {{Personalized next-song recommendation in online karaokes}},
url = {http://dl.acm.org/citation.cfm?doid=2507157.2507215},
year = {2013}
}

